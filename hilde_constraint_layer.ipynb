{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1VqX+2lkhI/4nE9JdtXeK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjana1976/-hilde-dynamic-constraints/blob/master/hilde_constraint_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnccNNIW6xC3"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall torch torchvision torchaudio -y\n",
        "#!pip cache purge\n",
        "#!apt-get update\n",
        "#!apt-get install -y python3-pip\n",
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers>=4.30.0 openai>=1.0.0 numpy>=1.24.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchaudio\n",
        "import transformers\n",
        "import openai\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"Torchaudio version: {torchaudio.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"OpenAI version: {openai.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device:\", torch.cuda.get_device_name() if torch.cuda.is_available() else \"None\")\n"
      ],
      "metadata": {
        "id": "RxMX5C8N78OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PeOeI5kd4pHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ask the user for their API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "# Store it as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Optional: confirm it's set (don't print the key itself for safety)\n",
        "if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    print(\"API key set successfully!\")\n",
        "else:\n",
        "    print(\"API key not set.\")"
      ],
      "metadata": {
        "id": "WBEfjfp38j8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# HiLDe Completion Engine\n",
        "\n",
        "# Simplified code generation using Qwen2.5-Coder-7B\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "class HiLDeLiteCompletionEngine:\n",
        "    def __init__(self, model_name=\"Qwen/Qwen2.5-Coder-7B-Instruct\"):\n",
        "        #Initialize the completion engine with Qwen2.5-Coder-7B\n",
        "        print(f\"Loading completion model: {model_name}\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Load model with optimizations for Colab\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        print(\"Completion engine loaded successfully!\")\n",
        "\n",
        "    def generate_completion(self, prompt, max_tokens=100, temperature=0.7):\n",
        "\n",
        "       # Generate a single code completion\n",
        "       # Arguments:\n",
        "          #  prompt (str): The input prompt\n",
        "          #  max_tokens (int): Maximum tokens to generate\n",
        "          # temperature (float): Sampling temperature\n",
        "        #Returns:\n",
        "            #str: Generated code completion\n",
        "\n",
        "        try:\n",
        "            # Tokenize input\n",
        "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "            inputs = inputs.to(self.model.device)\n",
        "\n",
        "            # Generate completion\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs,\n",
        "                    max_new_tokens=max_tokens,\n",
        "                    temperature=temperature,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            # Decode output\n",
        "            completion = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Remove the original prompt from completion\n",
        "            if completion.startswith(prompt):\n",
        "                completion = completion[len(prompt):].strip()\n",
        "\n",
        "            return completion\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating completion: {e}\")\n",
        "            return \"# Error: Could not generate completion\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the completion engine\n",
        "    engine = HiLDeLiteCompletionEngine()\n",
        "\n",
        "    test_prompt = \"def hash_password(password):\"\n",
        "    print(f\"Prompt: {test_prompt}\")\n",
        "\n",
        "    completion = engine.generate_completion(test_prompt, max_tokens=50)\n",
        "    print(f\"Generated: {completion}\")\n"
      ],
      "metadata": {
        "id": "3o5XLdJo-b43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Dynamic Rule Translator\n",
        "\n",
        "#Converts natural language constraints to complete rule specifications using LLM.\n",
        "#This replaces the hardcoded dictionary approach with  dynamic rule generation.\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import openai\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "\n",
        "class DynamicRuleTranslator:\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        #Initialize the rule translator\n",
        "        if not api_key:\n",
        "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "        if api_key:\n",
        "            openai.api_key = api_key\n",
        "            self.api_key = api_key\n",
        "        else:\n",
        "            print(\"Warning: No API key provided. Using fallback translation.\")\n",
        "            self.api_key = None\n",
        "\n",
        "    def translate_constraints(self, user_constraints: List[str])-> List[Dict[str, Any]]:\n",
        "\n",
        "       # Translate natural language constraints to complete rule specifications\n",
        "\n",
        "        #Args:\n",
        "          #user_constraints: List of natural language constraint descriptions\n",
        "        # Returns:\n",
        "         #  List of complete rule dictionaries with all necessary information\n",
        "\n",
        "        translated_rules = []\n",
        "\n",
        "        for constraint in user_constraints:\n",
        "            rule_spec = self._translate_single_constraint(constraint)\n",
        "            if rule_spec:\n",
        "                translated_rules.append(rule_spec)\n",
        "\n",
        "        return translated_rules\n",
        "\n",
        "    def _translate_single_constraint(self, constraint: str) :\n",
        "        #Translate a single constraint using LLM\n",
        "        if self.api_key:\n",
        "            try:\n",
        "                return self._llm_translate_constraint(constraint)\n",
        "            except Exception as e:\n",
        "                print(f\"LLM translation error: {e}\")\n",
        "                return self._fallback_translate(constraint)\n",
        "        else:\n",
        "            return self._fallback_translate(constraint)\n",
        "\n",
        "    def _llm_translate_constraint(self, constraint: str) -> Dict[str, Any]:\n",
        "\n",
        "       # Use LLM to generate complete rule specification from natural language\n",
        "\n",
        "        prompt = f\"\"\"You are a code analysis expert. Convert this natural language constraint into a complete rule specification.\n",
        "\n",
        "User constraint: \"{constraint}\"\n",
        "\n",
        "Return a JSON object with the following structure:\n",
        "{{\n",
        "    \"rule_name\": \"snake_case_rule_name\",\n",
        "    \"ast_node_type\": \"ASTNodeType\",\n",
        "    \"message\": \"Human readable violation message\",\n",
        "    \"severity\": \"error|warning|info\",\n",
        "    \"description\": \"Brief description of what this rule checks\",\n",
        "    \"additional_checks\": {{\n",
        "        \"function_name\": \"optional_function_name\",\n",
        "        \"module_name\": \"optional_module_name\",\n",
        "        \"attribute_name\": \"optional_attribute_name\"\n",
        "    }}\n",
        "}}\n",
        "\n",
        "Rules for translation:\n",
        "1. rule_name: Use snake_case, be specific and technical\n",
        "2. ast_node_type: Use the exact Python AST node type name (e.g., \"While\", \"For\", \"Call\", \"Import\", etc.)\n",
        "3. message: Clear, actionable message for developers\n",
        "4. severity: \"error\" for critical violations, \"warning\" for style issues, \"info\" for suggestions\n",
        "5. description: Brief explanation of what the rule checks\n",
        "6. additional_checks: Optional object for specific checks\n",
        "\n",
        "Common patterns:\n",
        "- \"no while loops\" → ast_node_type: \"While\"\n",
        "- \"no function calls\" → ast_node_type: \"Call\"\n",
        "- \"no global variables\" → ast_node_type: \"Assign\" (at module level)\n",
        "- \"no imports\" → ast_node_type: \"Import\"\n",
        "- \"no classes\" → ast_node_type: \"ClassDef\"\n",
        "- \"no eval\" → ast_node_type: \"Call\", additional_checks.function_name: \"eval\"\n",
        "- \"no requests library\" → ast_node_type: \"Import\", additional_checks.module_name: \"requests\"\n",
        "\n",
        "Return ONLY the JSON object, no other text or explanation.\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a code analysis expert. Convert natural language constraints to complete technical rule specifications.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=300,\n",
        "                temperature=0.1,\n",
        "            )\n",
        "\n",
        "            result_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Clean up markdown (if LLM adds it)\n",
        "            if result_text.startswith(\"```json\"):\n",
        "                result_text = result_text.removeprefix(\"```json\").strip()\n",
        "            if result_text.endswith(\"```\"):\n",
        "                result_text = result_text.removesuffix(\"```\").strip()\n",
        "\n",
        "            rule_spec = json.loads(result_text)\n",
        "\n",
        "            required_fields = [\"rule_name\", \"ast_node_type\", \"message\", \"severity\"]\n",
        "            for field in required_fields:\n",
        "                if field not in rule_spec:\n",
        "                    raise ValueError(f\"Missing required field: {field}\")\n",
        "\n",
        "            if \"additional_checks\" not in rule_spec:\n",
        "                rule_spec[\"additional_checks\"] = {}\n",
        "\n",
        "            return rule_spec\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON parsing error: {e}\")\n",
        "            print(f\"LLM response: {result_text}\")\n",
        "            return self._fallback_translate(constraint)\n",
        "        except Exception as e:\n",
        "            print(f\"LLM translation error: {e}\")\n",
        "            return self._fallback_translate(constraint)\n",
        "\n",
        "    def _fallback_translate(self, constraint: str) -> Dict[str, Any]:\n",
        "        #Fallback translation when LLM is unavailable\n",
        "        constraint_lower = constraint.lower().strip()\n",
        "\n",
        "        fallback_rules = {\n",
        "            \"while\": {\"ast_node_type\": \"While\", \"message\": \"While loops are not allowed\", \"severity\": \"error\"},\n",
        "            \"for\": {\"ast_node_type\": \"For\", \"message\": \"For loops are not allowed\", \"severity\": \"error\"},\n",
        "            \"function call\": {\"ast_node_type\": \"Call\", \"message\": \"Function calls are not allowed\", \"severity\": \"error\"},\n",
        "            \"global\": {\"ast_node_type\": \"Assign\", \"message\": \"Global variables are not allowed\", \"severity\": \"warning\"},\n",
        "            \"import\": {\"ast_node_type\": \"Import\", \"message\": \"Import statements are not allowed\", \"severity\": \"error\"},\n",
        "            \"class\": {\"ast_node_type\": \"ClassDef\", \"message\": \"Class definitions are not allowed\", \"severity\": \"error\"},\n",
        "            \"recursion\": {\"ast_node_type\": \"Call\", \"message\": \"Recursive calls are not allowed\", \"severity\": \"error\"},\n",
        "            \"lambda\": {\"ast_node_type\": \"Lambda\", \"message\": \"Lambda functions are not allowed\", \"severity\": \"warning\"},\n",
        "            \"eval\": {\"ast_node_type\": \"Call\", \"message\": \"Eval function is not allowed\", \"severity\": \"error\"},\n",
        "        }\n",
        "\n",
        "        for keyword, rule_info in fallback_rules.items():\n",
        "            if keyword in constraint_lower:\n",
        "                rule_name = constraint.replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
        "                return {\n",
        "                    \"rule_name\": rule_name,\n",
        "                    \"ast_node_type\": rule_info[\"ast_node_type\"],\n",
        "                    \"message\": rule_info[\"message\"],\n",
        "                    \"severity\": rule_info[\"severity\"],\n",
        "                    \"description\": f\"Fallback rule for: {constraint}\",\n",
        "                    \"additional_checks\": {}\n",
        "                }\n",
        "\n",
        "        # Default fallback\n",
        "        rule_name = constraint.replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
        "        return {\n",
        "            \"rule_name\": rule_name,\n",
        "            \"ast_node_type\": \"Call\",\n",
        "            \"message\": f\"Violation of constraint: {constraint}\",\n",
        "            \"severity\": \"warning\",\n",
        "            \"description\": f\"Fallback rule for: {constraint}\",\n",
        "            \"additional_checks\": {}\n",
        "        }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the dynamic rule translator\n",
        "    translator = DynamicRuleTranslator()\n",
        "\n",
        "    test_constraints = [\n",
        "        \"don't allow while loops\",\n",
        "        \"no function calls\",\n",
        "        \"ban global variables\",\n",
        "        \"no imports please\",\n",
        "        \"don't use eval function\",\n",
        "        \"no requests library\"\n",
        "    ]\n",
        "\n",
        "    print(\"Testing Dynamic Rule Translator\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for constraint in test_constraints:\n",
        "        print(f\"\\nConstraint: '{constraint}'\")\n",
        "        rule_spec = translator._translate_single_constraint(constraint)\n",
        "        print(f\"Generated Rule:\")\n",
        "        print(json.dumps(rule_spec, indent=2))\n"
      ],
      "metadata": {
        "id": "zndoWcfDFEF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic Dynamic AST Checker\n",
        "#\n",
        "# Finds violations in Python code based on dynamic rule specifications.\n",
        "# This is now truly generic - no hardcoded rule mappings required!\n",
        "\n",
        "import ast\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "\n",
        "def find_violations_in_ast(source_code: str, rule_specifications: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    # Traverse AST to find violations based on dynamic rule specifications\n",
        "    #\n",
        "    # Args:\n",
        "    #     source_code: Python code to analyze\n",
        "    #     rule_specifications: List of complete rule dictionaries from DynamicRuleTranslator\n",
        "    #\n",
        "    # Returns:\n",
        "    #     List of violation dictionaries\n",
        "    violations = []\n",
        "\n",
        "    try:\n",
        "        tree = ast.parse(source_code)\n",
        "    except SyntaxError as e:\n",
        "        return [{\n",
        "            \"rule_name\": \"syntax_error\",\n",
        "            \"line\": e.lineno or 0,\n",
        "            \"column\": e.offset or 0,\n",
        "            \"message\": f\"Syntax error: {e.msg}\",\n",
        "            \"severity\": \"error\",\n",
        "            \"code_snippet\": \"\",\n",
        "            \"description\": \"Python syntax error in the code\"\n",
        "        }]\n",
        "\n",
        "    # Create visitor to check rule specifications\n",
        "    visitor = DynamicConstraintVisitor(rule_specifications, source_code)\n",
        "    visitor.visit(tree)\n",
        "\n",
        "    return visitor.violations\n",
        "\n",
        "\n",
        "class DynamicConstraintVisitor(ast.NodeVisitor):\n",
        "    # Generic AST visitor that checks for violations based on dynamic rule specifications\n",
        "\n",
        "    def __init__(self, rule_specifications: List[Dict[str, Any]], source_code: str):\n",
        "        self.rule_specifications = rule_specifications\n",
        "        self.source_code = source_code\n",
        "        self.source_lines = source_code.split('\\n')\n",
        "        self.violations = []\n",
        "        self.current_line = 0\n",
        "        self.function_names = set()  # Track function names for recursion detection\n",
        "        self.current_function = None  # Track current function for recursion detection\n",
        "\n",
        "    def visit(self, node):\n",
        "        # Visit each node and check all rule specifications\n",
        "        self.current_line = getattr(node, 'lineno', 0)\n",
        "\n",
        "        # Track function definitions for recursion detection\n",
        "        if isinstance(node, ast.FunctionDef):\n",
        "            self.function_names.add(node.name)\n",
        "            self.current_function = node.name\n",
        "\n",
        "        # Check each rule specification against this node\n",
        "        for rule_spec in self.rule_specifications:\n",
        "            if self._check_rule_violation(node, rule_spec):\n",
        "                violation = self._create_violation(node, rule_spec)\n",
        "                self.violations.append(violation)\n",
        "\n",
        "        # Continue visiting child nodes\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def _check_rule_violation(self, node: ast.AST, rule_spec: Dict[str, Any]) -> bool:\n",
        "        # Check if a node violates a specific rule specification\n",
        "        #\n",
        "        # This is the key improvement: we now read the ast_node_type directly\n",
        "        # from the rule specification instead of using hardcoded mappings!\n",
        "        expected_node_type = rule_spec.get('ast_node_type', '')\n",
        "        rule_name = rule_spec.get('rule_name', '')\n",
        "        additional_checks = rule_spec.get('additional_checks', {})\n",
        "\n",
        "        # Check if current node matches the expected AST node type\n",
        "        if not expected_node_type or type(node).__name__ != expected_node_type:\n",
        "            return False\n",
        "\n",
        "        # Perform additional checks if specified\n",
        "        if additional_checks:\n",
        "            return self._perform_additional_checks(node, rule_spec, additional_checks)\n",
        "\n",
        "        # For rules that need special logic beyond just node type matching\n",
        "        if rule_name.endswith('_global_vars'):\n",
        "            return self._is_global_assignment(node)\n",
        "        elif rule_name.endswith('_recursion'):\n",
        "            return self._is_recursive_call(node)\n",
        "        elif rule_name.endswith('_function_calls'):\n",
        "            return True  # Any function call is a violation\n",
        "        else:\n",
        "            return True  # Node type matches, so it's a violation\n",
        "\n",
        "    def _perform_additional_checks(self, node: ast.AST, rule_spec: Dict[str, Any], additional_checks: Dict[str, Any]) -> bool:\n",
        "        # Perform additional checks based on the rule specification\n",
        "\n",
        "        # Check for specific function names (e.g., \"no eval\")\n",
        "        if 'function_name' in additional_checks:\n",
        "            expected_function = additional_checks['function_name']\n",
        "            if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):\n",
        "                return node.func.id == expected_function\n",
        "\n",
        "        # Check for specific module names (e.g., \"no requests library\")\n",
        "        if 'module_name' in additional_checks:\n",
        "            expected_module = additional_checks['module_name']\n",
        "            if isinstance(node, ast.Import):\n",
        "                for alias in node.names:\n",
        "                    if alias.name == expected_module:\n",
        "                        return True\n",
        "            elif isinstance(node, ast.ImportFrom):\n",
        "                if node.module == expected_module:\n",
        "                    return True\n",
        "\n",
        "        # Check for specific attribute names\n",
        "        if 'attribute_name' in additional_checks:\n",
        "            expected_attribute = additional_checks['attribute_name']\n",
        "            if isinstance(node, ast.Attribute):\n",
        "                return node.attr == expected_attribute\n",
        "\n",
        "        # If no specific additional checks match, return True (node type violation)\n",
        "        return True\n",
        "\n",
        "    def _is_global_assignment(self, node: ast.Assign) -> bool:\n",
        "        # Check if assignment is at module level (global)\n",
        "        if not isinstance(node, ast.Assign):\n",
        "            return False\n",
        "\n",
        "        # Simple heuristic: if we're at the top level of the AST\n",
        "        # In a more sophisticated implementation, you'd track the AST depth\n",
        "        return True\n",
        "\n",
        "    def _is_recursive_call(self, node: ast.Call) -> bool:\n",
        "        # Check if function call is recursive\n",
        "        if not isinstance(node, ast.Call) or not isinstance(node.func, ast.Name):\n",
        "            return False\n",
        "\n",
        "        # Check if the function being called is the same as the current function\n",
        "        if self.current_function and node.func.id == self.current_function:\n",
        "            return True\n",
        "\n",
        "        # Check if the function being called is in our known function names\n",
        "        # (This is a simple heuristic - a more sophisticated approach would\n",
        "        # analyze the actual call graph)\n",
        "        if node.func.id in self.function_names:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _create_violation(self, node: ast.AST, rule_spec: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        # Create a violation dictionary from the rule specification\n",
        "        rule_name = rule_spec.get('rule_name', 'unknown_rule')\n",
        "        message = rule_spec.get('message', f'Violation of {rule_name}')\n",
        "        severity = rule_spec.get('severity', 'warning')\n",
        "        description = rule_spec.get('description', '')\n",
        "\n",
        "        # Get code snippet\n",
        "        code_snippet = \"\"\n",
        "        if hasattr(node, 'lineno') and node.lineno:\n",
        "            line_idx = node.lineno - 1\n",
        "            if 0 <= line_idx < len(self.source_lines):\n",
        "                code_snippet = self.source_lines[line_idx].strip()\n",
        "\n",
        "        return {\n",
        "            \"rule_name\": rule_name,\n",
        "            \"line\": getattr(node, 'lineno', 0),\n",
        "            \"column\": getattr(node, 'col_offset', 0),\n",
        "            \"message\": message,\n",
        "            \"severity\": severity,\n",
        "            \"code_snippet\": code_snippet,\n",
        "            \"description\": description,\n",
        "            \"ast_node_type\": type(node).__name__\n",
        "        }\n",
        "\n",
        "\n",
        "def get_available_ast_node_types() -> List[str]:\n",
        "    # Get list of available Python AST node types for reference\n",
        "    return [\n",
        "        \"While\", \"For\", \"Call\", \"Import\", \"ImportFrom\", \"Assign\", \"ClassDef\",\n",
        "        \"FunctionDef\", \"Lambda\", \"ListComp\", \"DictComp\", \"GeneratorExp\",\n",
        "        \"IfExp\", \"Assert\", \"Raise\", \"Try\", \"With\", \"AsyncFunctionDef\",\n",
        "        \"AsyncFor\", \"AsyncWith\", \"Name\", \"Attribute\", \"BinOp\", \"UnaryOp\",\n",
        "        \"Compare\", \"BoolOp\", \"If\", \"Return\", \"Yield\", \"YieldFrom\", \"Delete\",\n",
        "        \"AugAssign\", \"Global\", \"Nonlocal\", \"Pass\", \"Break\", \"Continue\"\n",
        "    ]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the dynamic AST checker\n",
        "    test_code = \"\"\"\n",
        "def test_function():\n",
        "    import os\n",
        "    global_var = \"test\"\n",
        "\n",
        "    while True:\n",
        "        result = calculate(5)\n",
        "        if result:\n",
        "            break\n",
        "\n",
        "    eval(\"print('hello')\")\n",
        "    return result\n",
        "\"\"\"\n",
        "\n",
        "    # Example rule specifications (what the DynamicRuleTranslator would generate)\n",
        "    test_rule_specs = [\n",
        "        {\n",
        "            \"rule_name\": \"no_while_loops\",\n",
        "            \"ast_node_type\": \"While\",\n",
        "            \"message\": \"While loops are not allowed\",\n",
        "            \"severity\": \"error\",\n",
        "            \"description\": \"Checks for while loop usage\",\n",
        "            \"additional_checks\": {}\n",
        "        },\n",
        "        {\n",
        "            \"rule_name\": \"no_global_vars\",\n",
        "            \"ast_node_type\": \"Assign\",\n",
        "            \"message\": \"Global variables are not allowed\",\n",
        "            \"severity\": \"warning\",\n",
        "            \"description\": \"Checks for global variable assignments\",\n",
        "            \"additional_checks\": {}\n",
        "        },\n",
        "        {\n",
        "            \"rule_name\": \"no_imports\",\n",
        "            \"ast_node_type\": \"Import\",\n",
        "            \"message\": \"Import statements are not allowed\",\n",
        "            \"severity\": \"error\",\n",
        "            \"description\": \"Checks for import statements\",\n",
        "            \"additional_checks\": {}\n",
        "        },\n",
        "        {\n",
        "            \"rule_name\": \"no_eval_function\",\n",
        "            \"ast_node_type\": \"Call\",\n",
        "            \"message\": \"Eval function is not allowed\",\n",
        "            \"severity\": \"error\",\n",
        "            \"description\": \"Checks for eval function usage\",\n",
        "            \"additional_checks\": {\n",
        "                \"function_name\": \"eval\"\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"Testing Dynamic AST Checker\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Test code:\")\n",
        "    print(test_code)\n",
        "    print(\"\\nRule specifications:\")\n",
        "    for spec in test_rule_specs:\n",
        "        print(f\"- {spec['rule_name']}: {spec['ast_node_type']}\")\n",
        "\n",
        "    violations = find_violations_in_ast(test_code, test_rule_specs)\n",
        "\n",
        "    print(f\"\\nViolations found: {len(violations)}\")\n",
        "    for violation in violations:\n",
        "        print(f\"- {violation['rule_name']} (Line {violation['line']}, {violation['severity']}): {violation['message']}\")\n",
        "        if violation['code_snippet']:\n",
        "            print(f\"  Code: {violation['code_snippet']}\")\n"
      ],
      "metadata": {
        "id": "67G--DcgFzL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# HiLDe Analysis Engine\n",
        "\n",
        "# GPT-4 powered summarization of constraint violations, API KEY NEEDED FOR THIS PART\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "class GPT4AnalysisEngine:\n",
        "    def __init__(self, api_key=None):\n",
        "        # Initialize the GPT-4 analysis engine\n",
        "        if not api_key:\n",
        "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "        if api_key:\n",
        "            self.client = OpenAI(api_key=api_key)\n",
        "            self.api_key = api_key\n",
        "        else:\n",
        "          #Fall back plan\n",
        "            print(\"Warning: No API key provided. Using fallback summaries.\")\n",
        "            self.client = None\n",
        "            self.api_key = None\n",
        "\n",
        "    def summarize_violations(self, violations: List[Dict[str, Any]]) :\n",
        "\n",
        "       # Summarize constraint violations using GPT-4\n",
        "       # Args:\n",
        "           # violations: List of violation dictionaries\n",
        "      # Returns:\n",
        "           # str: Human-readable summary\n",
        "\n",
        "        if not violations:\n",
        "            return \"No constraint violations found. Code looks good!\"\n",
        "\n",
        "        try:\n",
        "            if self.client:\n",
        "                return self._gpt4_summary(violations)\n",
        "            else:\n",
        "                return self._create_fallback_summary(violations)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in GPT-4 analysis: {e}\")\n",
        "            return self._create_fallback_summary(violations)\n",
        "\n",
        "    def _gpt4_summary(self, violations: List[Dict[str, Any]]):\n",
        "        \"\"\"Generate summary using GPT-4 API\"\"\"\n",
        "        violations_json = json.dumps(violations, indent=2)\n",
        "\n",
        "        prompt = f\"\"\"Act as a senior software engineer and code quality expert. The following JSON contains constraint violations found in a Python code snippet:\n",
        "\n",
        "{violations_json}\n",
        "\n",
        "Please provide a concise, high-level summary that:\n",
        "1. Explains what problems were found\n",
        "2. Suggests how to fix each issue\n",
        "3. Uses clear, non-technical language suitable for developers\n",
        "4. Focuses on the most critical issues first\n",
        "\n",
        "Format your response as a clear, actionable summary. Do not include a closing or sign-off like \"Best\" or a name. be direct and formal\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a senior software engineer and code quality expert.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=500,\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"GPT-4 API error: {e}\")\n",
        "            return self._create_fallback_summary(violations)\n",
        "\n",
        "    def _create_fallback_summary(self, violations: List[Dict[str, Any]]) :\n",
        "        # basic summary when GPT-4 is unavailable\n",
        "        if not violations:\n",
        "            return \"No violations found.\"\n",
        "\n",
        "        summary_parts = [f\"Found {len(violations)} constraint violation(s):\"]\n",
        "\n",
        "        for i, violation in enumerate(violations, 1):\n",
        "            rule = violation.get('rule_name', 'Unknown rule')\n",
        "            line = violation.get('line', 'Unknown line')\n",
        "            message = violation.get('message', 'No message')\n",
        "            severity = violation.get('severity', 'info')\n",
        "\n",
        "            summary_parts.append(f\"{i}. **{rule}** (Line {line}, {severity}): {message}\")\n",
        "\n",
        "        return \"\\n\".join(summary_parts)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the analysis engine\n",
        "    engine = GPT4AnalysisEngine()\n",
        "\n",
        "    test_violations = [\n",
        "        {\n",
        "            \"rule_name\": \"no_while_loops\",\n",
        "            \"line\": 5,\n",
        "            \"message\": \"While loops are not allowed\",\n",
        "            \"severity\": \"error\"\n",
        "        },\n",
        "        {\n",
        "            \"rule_name\": \"no_global_vars\",\n",
        "            \"line\": 3,\n",
        "            \"message\": \"Global variables are not allowed\",\n",
        "            \"severity\": \"warning\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    summary = engine.summarize_violations(test_violations)\n",
        "    print(\"Testing....Summary:\")\n",
        "    print(summary)\n"
      ],
      "metadata": {
        "id": "g6uKlj9Nq6EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dynamic Constraint System\n",
        "\n",
        "# Integrates completion, dynamic rule translation, and generic AST checking.\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "\n",
        "\n",
        "class DynamicConstraintSystem:\n",
        "    def __init__(self, completion_engine, analysis_engine, api_key=None):\n",
        "        self.completion_engine = completion_engine\n",
        "        self.analysis_engine = analysis_engine\n",
        "        self.rule_translator = DynamicRuleTranslator(api_key)\n",
        "\n",
        "        print(\"Dynamic Constraint System initialized.\")\n",
        "\n",
        "    def process_code_with_constraints(self, prompt: str, user_constraints: List[str], max_tokens: int = 100) -> Dict[str, Any]:\n",
        "        print(\"Starting processing:\")\n",
        "\n",
        "        print(\"Generating code:\")\n",
        "        completion = self.completion_engine.generate_completion(prompt, max_tokens)\n",
        "\n",
        "        print(\"Translating constraints:\")\n",
        "        rule_specifications = self.rule_translator.translate_constraints(user_constraints)\n",
        "\n",
        "        print(\"Checking code:\")\n",
        "        violations = find_violations_in_ast(completion, rule_specifications)\n",
        "\n",
        "        print(\"Summarizing results:\")\n",
        "        summary = self.analysis_engine.summarize_violations(violations)\n",
        "\n",
        "        return {\n",
        "            \"completion\": completion,\n",
        "            \"user_constraints\": user_constraints,\n",
        "            \"rule_specifications\": rule_specifications,\n",
        "            \"violations\": violations,\n",
        "            \"summary\": summary,\n",
        "            \"violation_count\": len(violations),\n",
        "            \"constraints_checked\": [spec['rule_name'] for spec in rule_specifications]\n",
        "        }\n",
        "\n",
        "    def add_custom_constraint(self, constraint_description: str)-> Dict[str, Any]:\n",
        "        print(f\"Adding constraint: '{constraint_description}'\")\n",
        "\n",
        "        rule_specs = self.rule_translator.translate_constraints([constraint_description])\n",
        "        if rule_specs:\n",
        "            rule_spec = rule_specs[0]\n",
        "            print(f\"Rule: {rule_spec['rule_name']} ({rule_spec['ast_node_type']})\")\n",
        "            return rule_spec\n",
        "        else:\n",
        "            print(\"Failed to add constraint.\")\n",
        "            return {}\n",
        "\n",
        "    def test_constraint_on_code(self, code: str, constraint_description: str) -> Dict[str, Any]:\n",
        "        print(f\"Testing constraint '{constraint_description}' on code...\")\n",
        "\n",
        "        rule_specs = self.rule_translator.translate_constraints([constraint_description])\n",
        "        if not rule_specs:\n",
        "            return {\"error\": \"Failed to translate constraint\"}\n",
        "\n",
        "        violations = find_violations_in_ast(code, rule_specs)\n",
        "        summary = self.analysis_engine.summarize_violations(violations)\n",
        "\n",
        "        return {\n",
        "            \"constraint\": constraint_description,\n",
        "            \"rule_specification\": rule_specs[0],\n",
        "            \"violations\": violations,\n",
        "            \"summary\": summary,\n",
        "            \"violation_count\": len(violations)\n",
        "        }\n",
        "\n",
        "    def get_system_info(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"system_type\": \"Dynamic Constraint System\",\n",
        "            \"features\": [\n",
        "                \"Dynamic rule translation using LLM\",\n",
        "                \"Generic AST checking (no hardcoded rules)\",\n",
        "                \"Extensible constraint system\",\n",
        "                \"Natural language constraint input\",\n",
        "                \"Intelligent violation summarization\"\n",
        "            ],\n",
        "            \"capabilities\": [\n",
        "                \"Add new constraints without code changes\",\n",
        "                \"Support for any Python AST node type\",\n",
        "                \"Additional checks for specific functions/modules\",\n",
        "                \"Configurable severity levels\",\n",
        "                \"Human-readable violation messages\"\n",
        "            ],\n",
        "            \"example_constraints\": [\n",
        "                \"don't allow while loops\",\n",
        "                \"no function calls\",\n",
        "                \"ban global variables\",\n",
        "                \"no imports\",\n",
        "                \"don't use eval function\",\n",
        "                \"no requests library\",\n",
        "                \"no database connections\",\n",
        "                \"no file operations\",\n",
        "                \"no network requests\",\n",
        "                \"no recursion\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def demonstrate_extensibility(self):\n",
        "        print(\"Demonstrating dynamic system extensibility\")\n",
        "        print(\"===============================================================\")\n",
        "\n",
        "        new_constraints = [\n",
        "            \"no pandas library usage\",\n",
        "            \"don't allow database connections\",\n",
        "            \"ban file operations\",\n",
        "            \"no network requests\",\n",
        "            \"no threading\",\n",
        "            \"no multiprocessing\",\n",
        "            \"no subprocess calls\",\n",
        "            \"no os.system calls\",\n",
        "            \"no eval or exec\",\n",
        "            \"no pickle operations\"\n",
        "        ]\n",
        "\n",
        "        print(\"Adding new constraints:\")\n",
        "        for constraint in new_constraints:\n",
        "            rule_spec = self.add_custom_constraint(constraint)\n",
        "            if rule_spec:\n",
        "                print(f\"'{constraint}' → {rule_spec['ast_node_type']} ({rule_spec['severity']})\")\n",
        "            else:\n",
        "                print(f\"Failed to process: '{constraint}'\")\n",
        "\n",
        "        print(\"\\nAll constraints added without code changes.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing Dynamic Constraint System\")\n",
        "    print(\"===================================================================\")\n",
        "\n",
        "    class MockCompletionEngine:\n",
        "        def generate_completion(self, prompt, max_tokens):\n",
        "            return f\"\"\"def {prompt.split('(')[0].split()[-1] if '(' in prompt else 'test_function'}():\n",
        "    import os\n",
        "    global_var = \"test\"\n",
        "\n",
        "    while True:\n",
        "        result = calculate(5)\n",
        "        if result:\n",
        "            break\n",
        "\n",
        "    eval(\"print('hello')\")\n",
        "    return result\"\"\"\n",
        "\n",
        "    class MockAnalysisEngine:\n",
        "        def summarize_violations(self, violations):\n",
        "            if not violations:\n",
        "                return \"No violations found.\"\n",
        "\n",
        "            summary_parts = [f\"Found {len(violations)} violation(s):\"]\n",
        "            for i, violation in enumerate(violations, 1):\n",
        "                summary_parts.append(f\"{i}. {violation['rule_name']} (Line {violation['line']}, {violation['severity']}): {violation['message']}\")\n",
        "            return \"\\n\".join(summary_parts)\n",
        "\n",
        "    completion_engine = MockCompletionEngine()\n",
        "    analysis_engine = MockAnalysisEngine()\n",
        "    constraint_system = DynamicConstraintSystem(completion_engine, analysis_engine)\n",
        "\n",
        "    prompt = \"def process_data():\"\n",
        "    user_constraints = [\n",
        "        \"don't allow while loops\",\n",
        "        \"no function calls\",\n",
        "        \"ban global variables\",\n",
        "        \"no imports\",\n",
        "        \"don't use eval function\"\n",
        "    ]\n",
        "\n",
        "    result = constraint_system.process_code_with_constraints(prompt, user_constraints)\n",
        "\n",
        "    print(\"=== RESULT ===\")\n",
        "    print(f\"Generated Code:\\n{result['completion']}\\n\")\n",
        "    print(f\"User Constraints: {result['user_constraints']}\")\n",
        "    print(f\"Rules Generated: {len(result['rule_specifications'])}\")\n",
        "    for spec in result['rule_specifications']:\n",
        "        print(f\" - {spec['rule_name']}: {spec['ast_node_type']}\")\n",
        "    print(f\"Violations Found: {result['violation_count']}\")\n",
        "    print(\"Summary:\")\n",
        "    print(result['summary'])\n",
        "\n",
        "    print(\"\\n\" + \"============================================================\")\n",
        "    constraint_system.demonstrate_extensibility()\n"
      ],
      "metadata": {
        "id": "SkChyb3iGZ0A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}